(defpackage :gdrl-ch08
  (:use #:common-lisp
        #:mu
        #:th
        #:th.layers
        #:th.env
        #:th.env.examples))

(in-package :gdrl-ch08)

(defun fcq (input-dim output-dim)
  (let ((h1 5)
        (h2 5))
    (sequential-layer
     (affine-layer input-dim h1 :activation :tanh)
     (affine-layer h1 h2 :activation :tanh)
     (affine-layer h2 output-dim))))

(defun epsilon-greedy (model state &key (epsilon 0.1D0))
  (let ((qvs ($evaluate model state)))
    (if (> (random 1D0) epsilon)
        ($argmax qvs 1)
        (tensor.long ($bernoulli (tensor ($size state 0) 1) 0.5D0)))))

(defun loss (v r) ($mse v r))

(defun select-action (model state &key (epsilon 0.1D0))
  (-> (epsilon-greedy model (tensor (list (list state))) :epsilon epsilon)
      ($scalar)))

(let* ((env (slippery-walk-seven-env))
       (optres (env/value-iteration env :gamma 1D0))
       (opt-v (value-iteration/optimal-value-function optres))
       (opt-p (value-iteration/optimal-policy optres))
       (opt-q (value-iteration/optimal-action-value-function optres)))
  (env/print-state-value-function env opt-v :ncols 9)
  (env/print-policy env opt-p :action-symbols '("<" ">") :ncols 9)
  (prn opt-q))

(defun decay-schedule (v0 minv decay-ratio max-steps &key (log-start -2) (log-base 10))
  (let* ((decay-steps (round (* max-steps decay-ratio)))
         (rem-steps (- max-steps decay-steps))
         (vs (-> ($/ (logspace log-start 0 decay-steps) (log log-base 10))
                 ($list)
                 (reverse)
                 (tensor)))
         (minvs ($min vs))
         (maxvs ($max vs))
         (rngv (- maxvs minvs))
         (vs ($/ ($- vs minvs) rngv))
         (vs ($+ minv ($* vs (- v0 minv)))))
    ($cat vs ($fill! (tensor rem-steps) ($last vs)))))

(defun q-learning (env &key (gamma 1D0)
                         (alpha0 0.5) (min-alpha 0.01) (alpha-decay-ratio 0.5)
                         (epsilon0 1.0) (min-epsilon 0.1) (epsilon-decay-ratio 0.9)
                         (nepisodes 3000))
  (let* ((ns (env/state-count env))
         (na (env/action-count env))
         (pi-track '())
         (Q (zeros ns na))
         (Q-track (zeros nepisodes ns na))
         (alphas (decay-schedule alpha0 min-alpha alpha-decay-ratio nepisodes))
         (epsilons (decay-schedule epsilon0 min-epsilon epsilon-decay-ratio nepisodes))
         (select-action (lambda (Q state epsilon)
                          (if (> (random 1D0) epsilon)
                              ($argmax ($ Q state))
                              (random ($count ($ Q state)))))))
    (loop :for e :from 0 :below nepisodes
          :for state = (env/reset! env)
          :for eps = ($ epsilons e)
          :do (let ((done nil))
                (loop :while (not done)
                      :do (let* ((action (funcall select-action Q state eps))
                                 (tx (env/step! env action))
                                 (next-state (transition/next-state tx))
                                 (reward (transition/reward tx))
                                 (terminalp (transition/terminalp tx))
                                 (td-target (+ reward (* gamma ($max ($ Q next-state))
                                                         (if terminalp 0 1))))
                                 (td-error (- td-target ($ Q state action))))
                            (incf ($ Q state action) (* ($ alphas e) td-error))
                            (setf done terminalp
                                  state next-state)))
                (setf ($ Q-track e) Q)
                (push ($squeeze ($argmax Q 1)) pi-track)))
    (let ((v ($squeeze (car ($max* Q 1))))
          (va ($squeeze ($argmax Q 1))))
      (list Q v (lambda (s) ($ va s)) Q-track (reverse pi-track)))))

(let* ((env (th.env.examples:slippery-walk-seven-env))
       (res (q-learning env :gamma 1D0 :nepisodes 3000))
       (Q ($ res 0))
       (v ($ res 1))
       (policy ($ res 2)))
  (env/print-state-value-function env v :ncols 9)
  (env/print-policy env policy :action-symbols '("<" ">") :ncols 9)
  (prn Q))

(defparameter *model* (fcq 1 2))
(prn ($evaluate *model* (tensor '((0) (1) (2) (3) (4) (5) (6) (7) (8)))))
(prn ($argmax ($evaluate *model* (tensor '((0) (1) (2) (3) (4) (5) (6) (7) (8)))) 1))

(let* ((env (slippery-walk-seven-env))
       (optres (env/value-iteration env :gamma 1D0))
       (opt-q (value-iteration/optimal-action-value-function optres))
       (states (tensor '((0) (1) (2) (3) (4) (5) (6) (7) (8)))))
  (loop :repeat 5000
        :for k :from 1
        :do (let* ((v ($execute *model* states))
                   (l (loss v opt-q)))
              ($rmgd! *model*)
              (when (zerop (rem k 500)) (prn l))))
  (prn opt-q)
  (prn ($evaluate *model* states)))

(let ((env (slippery-walk-seven-env))
      (model *model*)
      (max-episodes 100)
      (nbatch 100)
      (ntrain 5)
      (epsilon 0.5D0)
      (gamma 1D0)
      (lr 0.01)
      (experiences '()))
  (loop :repeat max-episodes
        :for episode :from 1
        :do (let ((state (env/reset! env))
                  (done nil))
              (loop :while (not done)
                    :for action = (select-action model state :epsilon epsilon)
                    :for tx = (env/step! env action)
                    :for next-state = (transition/next-state tx)
                    :for reward = (transition/reward tx)
                    :for terminalp = (transition/terminalp tx)
                    :do (let ((experience (list state action reward next-state terminalp)))
                          (push experience experiences)
                          (let ((ne ($count experiences)))
                            (when (>= ne nbatch)
                              (let ((states (-> (mapcar #'$0 experiences)
                                                (tensor)
                                                ($reshape ne 1)))
                                    (actions (-> (mapcar #'$1 experiences)
                                                 (tensor.long)
                                                 ($reshape ne 1)))
                                    (rewards (-> (mapcar #'$2 experiences)
                                                 (tensor)
                                                 ($reshape ne 1)))
                                    (next-states (-> (mapcar #'$3 experiences)
                                                     (tensor)
                                                     ($reshape ne 1)))
                                    (facs (-> (mapcar (lambda (p) (if ($4 p) 0 1)) experiences)
                                              (tensor)
                                              ($reshape ne 1))))
                                (with-max-heap ()
                                  (loop :repeat ntrain
                                        :for k :from 1
                                        :for maxqs = (-> ($evaluate model next-states)
                                                         ($max 1))
                                        :for tqs = ($+ rewards ($* gamma maxqs facs))
                                        :for qsa = ($gather ($execute model states) 1 actions)
                                        :do (let* ((loss (loss qsa tqs))
                                                   (lv (format nil "~,4F" ($data loss)))
                                                   (lbl (format nil "[~4D/~2D]" episode k)))
                                              (when (zerop (rem episode 20))
                                                (when (eq k 1) (prn lbl lv))
                                                (when (eq k ntrain) (prn lbl lv)))
                                              ($rpgd! model lr)))))))
                          (setf state next-state
                                done terminalp))))))

;; for cartpole example check examples/etc/cartpole-nfq.lisp
;; following is copy of it.

(defconstant +gravity+ 9.8D0)
(defconstant +masscart+ 1D0)
(defconstant +masspole+ 0.1D0)
(defconstant +total-mass+ (+ +masscart+ +masspole+))
(defconstant +length+ 0.5D0)
(defconstant +polemass-length+ (* +masspole+ +length+))
(defconstant +force-mag+ 10D0)
(defconstant +tau+ 0.02D0)

(defconstant +x-success-range+ 2.4D0)
(defconstant +theta-success-range+ (/ (* 12 PI) 180D0))

(defconstant +x-threshold+ 2.4D0)
(defconstant +theta-threshold-radians+ (/ PI 2))
(defconstant +c-trans+ 0.01D0)

(defconstant +train-max-steps+ 100)
(defconstant +eval-max-steps+ 3000)

(defclass cartpole-regulator-env ()
  ((mode :initform nil :accessor env/mode)
   (step :initform 0 :accessor env/episode-step)
   (state :initform nil :accessor env/state)))

(defun cartpole-regulator-env (&optional (m :train))
  (let ((n (make-instance 'cartpole-regulator-env)))
    (setf (env/mode n) m)
    (env/reset! n)
    n))

(defmethod env/reset! ((env cartpole-regulator-env))
  (with-slots (mode state step) env
    (setf step 0)
    (setf state (if (eq mode :train)
                    (tensor (list (random/uniform -2.3D0 2.3D0)
                                  0
                                  (random/uniform -0.3 0.3)
                                  0))
                    (tensor (list (random/uniform -1D0 1D0)
                                  0
                                  (random/uniform -0.3 0.3)
                                  0))))
    state))

(defmethod env/step! ((env cartpole-regulator-env) action)
  (let* ((x ($0 (env/state env)))
         (xd ($1 (env/state env)))
         (th ($2 (env/state env)))
         (thd ($3 (env/state env)))
         (force (if (eq action 1) +force-mag+ (- +force-mag+)))
         (costh (cos th))
         (sinth (sin th))
         (tmp (/ (+ force (* +polemass-length+ thd thd sinth))
                 +total-mass+))
         (thacc (/ (- (* +gravity+ sinth) (* costh tmp))
                   (* +length+
                      (- 4/3 (/ (* +masspole+ costh costh) +total-mass+)))))
         (xacc (- tmp (/ (* +polemass-length+ thacc costh) +total-mass+)))
         (cost +c-trans+)
         (done nil)
         (blown nil))
    (incf (env/episode-step env))
    (incf x (* +tau+ xd))
    (incf xd (* +tau+ xacc))
    (incf th (* +tau+ thd))
    (incf thd (* +tau+ thacc))
    (cond ((or (< x (- +x-threshold+)) (> x +x-threshold+)
               (< th (- +theta-threshold-radians+)) (> th +theta-threshold-radians+))
           (setf cost 1D0
                 done T))
          ((and (> x (- +x-success-range+)) (< x +x-success-range+)
                (> th (- +theta-success-range+)) (< th +theta-success-range+))
           (setf cost 0D0
                 done nil))
          (T (setf cost +c-trans+
                   done nil)))
    (when (>= (env/episode-step env)
             (if (eq :train (env/mode env)) +train-max-steps+ +eval-max-steps+))
      (setf blown T))
    (let ((next-state (tensor (list x xd th thd))))
      (setf (env/state env) next-state)
      (list nil next-state cost done blown))))

(defun generate-goal-patterns (&optional (size 100))
  (list (tensor (loop :repeat size
                      :collect (list (random/uniform -0.05 0.05)
                                     (random/normal 0 1)
                                     (random/uniform (- +theta-success-range+)
                                                     +theta-success-range+)
                                     (random/normal 0 1)
                                     (random 2))))
        (zeros size 1)))

(defun collect-experiences (env &optional selector)
  (let ((rollout '())
        (episode-cost 0)
        (state (env/reset! env))
        (done nil)
        (blown nil))
    (loop :while (and (not done) (not blown))
          :for action = (if selector
                            (funcall selector state)
                            (random 2))
          :for tx = (env/step! env action)
          :do (let ((next-state ($1 tx))
                    (cost ($2 tx)))
                (setf done ($3 tx)
                      blown ($4 tx))
                (push (list state action cost next-state done) rollout)
                (incf episode-cost cost)
                (setf state next-state)))
    (list (reverse rollout) episode-cost)))

(defun model (&optional (ni 5) (no 1))
  (let ((h1 5)
        (h2 5))
    (sequential-layer
     (affine-layer ni h1 :weight-initializer :random-uniform)
     (affine-layer h1 h2 :weight-initializer :random-uniform)
     (affine-layer h2 no :weight-initializer :random-uniform))))

(defun best-action-selector (model)
  (lambda (state)
    (let* ((state ($reshape state 1 4))
           (qleft ($evaluate model ($concat state (zeros 1 1) 1)))
           (qright ($evaluate model ($concat state (ones 1 1) 1))))
      (if (>= ($ qleft 0 0) ($ qright 0 0)) 1 0))))

(defun generate-patterns (model experiences &optional (gamma 0.95D0))
  (let* ((nr ($count experiences))
         (state-list (mapcar #'$0 experiences))
         (states (-> (apply #'$concat state-list)
                     ($reshape! nr 4)))
         (actions (-> (tensor (mapcar #'$1 experiences))
                      ($reshape! nr 1)))
         (costs (-> (tensor (mapcar #'$2 experiences))
                    ($reshape! nr 1)))
         (next-states (-> (apply #'$concat (mapcar #'$3 experiences))
                          ($reshape! nr 4)))
         (dones (-> (tensor (mapcar (lambda (e) (if ($4 e) 1 0)) experiences))
                    ($reshape! nr 1)))
         (xs ($concat states actions 1))
         (qleft ($evaluate model ($concat next-states (zeros nr 1) 1)))
         (qright ($evaluate model ($concat next-states (ones nr 1) 1)))
         (qns ($min ($concat qleft qright 1) 1))
         (tqvs ($+ costs ($* gamma qns ($- 1 dones)))))
    (list xs tqvs)))

(defun train (model xs ts)
  (let* ((ys ($execute model xs))
         (loss ($mse ys ts)))
    ($rpgd! model)
    ($data loss)))

(defun evaluate (env model)
  (let ((state (env/reset! env))
        (ne 0)
        (done nil)
        (blown nil)
        (ecost 0D0)
        (selector (best-action-selector model)))
    (loop :while (and (not done) (not blown))
          :for step :from 0 :below +eval-max-steps+
          :for action = (funcall selector state)
          :for tx = (env/step! env action)
          :do (let ((next-state ($1 tx))
                    (cost ($2 tx)))
                (setf done ($3 tx)
                      blown ($4 tx))
                (incf ecost cost)
                (incf ne)
                (setf state next-state)))
    (list ne
          (and (>= ne (- +eval-max-steps+ 2)) (<= (abs ($0 state)) +x-success-range+))
          ecost)))

(defvar *init-experience* T)
(defvar *increment-experience* T)
(defvar *hint-to-goal* T)
(defvar *max-epochs* 300)

(defun report (epoch loss ntrain ctrain neval ceval success)
  (when (or success (zerop (rem epoch 20)))
    (let ((fmt "EPOCH ~4D | TRAIN ~3D / ~4,2F | EVAL ~4D / ~5,2F | TRAIN.LOSS ~,4F"))
      (prn (format nil fmt epoch ntrain ctrain neval ceval loss)))))

(with-max-heap ()
  (let* ((train-env (cartpole-regulator-env :train))
         (eval-env (cartpole-regulator-env :eval))
         (model (model))
         (experiences '())
         (total-cost 0)
         (success nil))
    (when *init-experience*
      (let* ((exsi (collect-experiences train-env))
             (exs (car exsi))
             (ecost (cadr exsi)))
        (setf experiences exs)
        (incf total-cost ecost)))
    (loop :for epoch :from 1 :to *max-epochs*
          :while (not success)
          :do (let ((ctrain 0)
                    (ntrain 0))
                (when *increment-experience*
                  (let* ((exsi (collect-experiences train-env (best-action-selector model)))
                         (exs (car exsi)))
                    (setf ctrain (cadr exsi))
                    (setf ntrain ($count exs))
                    (setf experiences (append experiences exs))
                    (incf total-cost ctrain)))
                (let* ((xys (generate-patterns model experiences 0.95D0))
                       (xs (car xys))
                       (ys (cadr xys)))
                  (when *hint-to-goal*
                    (let ((gxys (generate-goal-patterns)))
                      (setf xs ($concat xs (car gxys) 0))
                      (setf ys ($concat ys (cadr gxys) 0))))
                  (let* ((loss (train model xs ys))
                         (eres (evaluate eval-env model))
                         (neval ($0 eres))
                         (ceval ($2 eres)))
                    (setf success ($1 eres))
                    (report epoch loss ntrain ctrain neval ceval success)))))
    (when success
      (prn (format nil "*** TOTAL ~6D / ~4,2F" ($count experiences) total-cost)))))
