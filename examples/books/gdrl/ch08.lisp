(defpackage :gdrl-ch08
  (:use #:common-lisp
        #:mu
        #:th
        #:th.layers
        #:th.env
        #:th.env.examples
        #:th.env.cartpole))

(in-package :gdrl-ch08)

(defun fcq (input-dim output-dim)
  (let ((h1 5)
        (h2 5))
    (sequential-layer
     (affine-layer input-dim h1 :activation :tanh)
     (affine-layer h1 h2 :activation :tanh)
     (affine-layer h2 output-dim))))

(defun epsilon-greedy (model state &key (epsilon 0.1D0))
  (let ((qvs ($evaluate model state)))
    (if (> (random 1D0) epsilon)
        ($argmax qvs 1)
        (tensor.long ($bernoulli (tensor ($size state 0) 1) 0.5D0)))))

(defun loss (v r) ($mse v r))

(defun select-action (model state &key (epsilon 0.1D0))
  (-> (epsilon-greedy model (tensor (list (list state))) :epsilon epsilon)
      ($scalar)))

(let* ((env (slippery-walk-seven-env))
       (optres (env/value-iteration env :gamma 1D0))
       (opt-v (value-iteration/optimal-value-function optres))
       (opt-p (value-iteration/optimal-policy optres))
       (opt-q (value-iteration/optimal-action-value-function optres)))
  (env/print-state-value-function env opt-v :ncols 9)
  (env/print-policy env opt-p :action-symbols '("<" ">") :ncols 9)
  (prn opt-q))

(defun decay-schedule (v0 minv decay-ratio max-steps &key (log-start -2) (log-base 10))
  (let* ((decay-steps (round (* max-steps decay-ratio)))
         (rem-steps (- max-steps decay-steps))
         (vs (-> ($/ (logspace log-start 0 decay-steps) (log log-base 10))
                 ($list)
                 (reverse)
                 (tensor)))
         (minvs ($min vs))
         (maxvs ($max vs))
         (rngv (- maxvs minvs))
         (vs ($/ ($- vs minvs) rngv))
         (vs ($+ minv ($* vs (- v0 minv)))))
    ($cat vs ($fill! (tensor rem-steps) ($last vs)))))

(defun q-learning (env &key (gamma 1D0)
                         (alpha0 0.5) (min-alpha 0.01) (alpha-decay-ratio 0.5)
                         (epsilon0 1.0) (min-epsilon 0.1) (epsilon-decay-ratio 0.9)
                         (nepisodes 3000))
  (let* ((ns (env/state-count env))
         (na (env/action-count env))
         (pi-track '())
         (Q (zeros ns na))
         (Q-track (zeros nepisodes ns na))
         (alphas (decay-schedule alpha0 min-alpha alpha-decay-ratio nepisodes))
         (epsilons (decay-schedule epsilon0 min-epsilon epsilon-decay-ratio nepisodes))
         (select-action (lambda (Q state epsilon)
                          (if (> (random 1D0) epsilon)
                              ($argmax ($ Q state))
                              (random ($count ($ Q state)))))))
    (loop :for e :from 0 :below nepisodes
          :for state = (env/reset! env)
          :for eps = ($ epsilons e)
          :do (let ((done nil))
                (loop :while (not done)
                      :do (let* ((action (funcall select-action Q state eps))
                                 (tx (env/step! env action))
                                 (next-state (transition/next-state tx))
                                 (reward (transition/reward tx))
                                 (terminalp (transition/terminalp tx))
                                 (td-target (+ reward (* gamma ($max ($ Q next-state))
                                                         (if terminalp 0 1))))
                                 (td-error (- td-target ($ Q state action))))
                            (incf ($ Q state action) (* ($ alphas e) td-error))
                            (setf done terminalp
                                  state next-state)))
                (setf ($ Q-track e) Q)
                (push ($squeeze ($argmax Q 1)) pi-track)))
    (let ((v ($squeeze (car ($max* Q 1))))
          (va ($squeeze ($argmax Q 1))))
      (list Q v (lambda (s) ($ va s)) Q-track (reverse pi-track)))))

(let* ((env (th.env.examples:slippery-walk-seven-env))
       (res (q-learning env :gamma 1D0 :nepisodes 3000))
       (Q ($ res 0))
       (v ($ res 1))
       (policy ($ res 2)))
  (env/print-state-value-function env v :ncols 9)
  (env/print-policy env policy :action-symbols '("<" ">") :ncols 9)
  (prn Q))

(defparameter *model* (fcq 1 2))
(prn ($evaluate *model* (tensor '((0) (1) (2) (3) (4) (5) (6) (7) (8)))))
(prn ($argmax ($evaluate *model* (tensor '((0) (1) (2) (3) (4) (5) (6) (7) (8)))) 1))

(let* ((env (slippery-walk-seven-env))
       (optres (env/value-iteration env :gamma 1D0))
       (opt-q (value-iteration/optimal-action-value-function optres))
       (states (tensor '((0) (1) (2) (3) (4) (5) (6) (7) (8)))))
  (loop :repeat 5000
        :for k :from 1
        :do (let* ((v ($execute *model* states))
                   (l (loss v opt-q)))
              ($rmgd! *model*)
              (when (zerop (rem k 500)) (prn l))))
  (prn opt-q)
  (prn ($evaluate *model* states)))

(let ((env (slippery-walk-seven-env))
      (model *model*)
      (max-episodes 100)
      (nbatch 100)
      (ntrain 5)
      (epsilon 0.5D0)
      (gamma 1D0)
      (lr 0.01)
      (experiences '()))
  (loop :repeat max-episodes
        :for episode :from 1
        :do (let ((state (env/reset! env))
                  (done nil))
              (loop :while (not done)
                    :for action = (select-action model state :epsilon epsilon)
                    :for tx = (env/step! env action)
                    :for next-state = (transition/next-state tx)
                    :for reward = (transition/reward tx)
                    :for terminalp = (transition/terminalp tx)
                    :do (let ((experience (list state action reward next-state terminalp)))
                          (push experience experiences)
                          (let ((ne ($count experiences)))
                            (when (>= ne nbatch)
                              (let ((states (-> (mapcar #'$0 experiences)
                                                (tensor)
                                                ($reshape ne 1)))
                                    (actions (-> (mapcar #'$1 experiences)
                                                 (tensor.long)
                                                 ($reshape ne 1)))
                                    (rewards (-> (mapcar #'$2 experiences)
                                                 (tensor)
                                                 ($reshape ne 1)))
                                    (next-states (-> (mapcar #'$3 experiences)
                                                     (tensor)
                                                     ($reshape ne 1)))
                                    (facs (-> (mapcar (lambda (p) (if ($4 p) 0 1)) experiences)
                                              (tensor)
                                              ($reshape ne 1))))
                                (with-max-heap ()
                                  (loop :repeat ntrain
                                        :for k :from 1
                                        :for maxqs = (-> ($evaluate model next-states)
                                                         ($max 1))
                                        :for tqs = ($+ rewards ($* gamma maxqs facs))
                                        :for qsa = ($gather ($execute model states) 1 actions)
                                        :do (let* ((loss (loss qsa tqs))
                                                   (lv (format nil "~,4F" ($data loss)))
                                                   (lbl (format nil "[~4D/~2D]" episode k)))
                                              (when (zerop (rem episode 20))
                                                (when (eq k 1) (prn lbl lv))
                                                (when (eq k ntrain) (prn lbl lv)))
                                              ($rpgd! model lr)))))))
                          (setf state next-state
                                done terminalp))))))

;; XXX need to tackle simpler problems than cartpole

(defun fcq (input-dim output-dim)
  (let ((h1 512)
        (h2 128))
    (sequential-layer
     (affine-layer input-dim h1)
     (affine-layer h1 h2)
     (affine-layer h2 output-dim))))

(defun epsilon-greedy (model state &key (epsilon 0.1D0))
  (let ((qvs ($evaluate model state)))
    (if (> (random 1D0) epsilon)
        ($argmax qvs 1)
        (tensor.long ($bernoulli (tensor ($size state 0) 1) 0.5D0)))))

;; xxx i don't know why $mse does not work
(defun loss (v r) ($mse v r))

(let* ((w ($parameter (zeros 4 2)))
       (a '((0) (1) (0) (1)))
       (r (ones 4 1)))
  (loop :repeat 3
        :for iter :from 0
        :for l = (loss ($gather w 1 a) r)
        :do (progn
              ($rmgd! w 0.1)
              (prn iter ":" ($data l)))))

(let* ((m (fcq 4 2))
       (x ($uniform (tensor 1 4) -0.05 0.05)))
  (epsilon-greedy m x))

(let* ((net (fcq 4 2))
       (x ($uniform (tensor 10 4) -0.05 0.05))
       (q ($evaluate net x))
       (dones (zeros 10 1))
       (rewards (ones 10 1))
       (gamma 0.99)
       (maxqs ($* (car ($max q 1)) ($- 1 dones)))
       (target-qs ($+ rewards ($* gamma maxqs)))
       (actions '((0) (0) (0) (1) (1) (1) (0) (0) (0) (1)))
       (qsa ($gather ($execute net x) 1 actions)))
  (list target-qs qsa))

(let ((env (cartpole-env))
      (n 10))
  (env/reset! env)
  (let ((states (->> (loop :for i :from 0 :below n
                           :for action = (random 2)
                           :collect (env/step! env action))
                     (mapcar #'cadr))))
    (-> (apply #'$concat states)
        ($reshape! n 4))))

(defparameter *model* (fcq 4 2))

(let ((env (cartpole-env))
      (model *model*)
      (max-episodes 10000)
      (nbatch 1024)
      (ntrain 40)
      (epsilon 0.5D0)
      (gamma 1D0)
      (lr 0.001)
      (experiences '()))
  (loop :repeat max-episodes
        :for episode :from 1
        :for state = (env/reset! env)
        :do (let ((done nil))
              (loop :while (not done)
                    :for action = (-> (epsilon-greedy model ($reshape state 1 4) :epsilon epsilon)
                                      ($ 0 0))
                    :for tx = (env/step! env action)
                    :for next-state = (transition/next-state tx)
                    :for reward = (transition/reward tx)
                    :for terminalp = (transition/terminalp tx)
                    :do (let ((e (list state action reward next-state terminalp)))
                          (push e experiences)
                          (let ((ne ($count experiences)))
                            (when (>= ne nbatch)
                              (let* ((states (-> (apply #'$concat (mapcar #'$0 experiences))
                                                 ($reshape ne 4)))
                                     (actions (-> (mapcar #'$1 experiences)
                                                  (tensor.long)
                                                  ($reshape ne 1)))
                                     (rewards (-> (mapcar #'$2 experiences)
                                                  (tensor)
                                                  ($reshape ne 1)))
                                     (next-states (-> (apply #'$concat (mapcar #'$3 experiences))
                                                      ($reshape ne 4)))
                                     (donesf (-> (mapcar (lambda (p) (if ($4 p) 0 1)) experiences)
                                                 (tensor)
                                                 ($reshape ne 1))))
                                (with-max-heap ()
                                  (loop :repeat ntrain
                                        :for k :from 1
                                        :for maxqs = (-> ($evaluate model next-states)
                                                         ($max 1)
                                                         (car))
                                        :for tqs = ($+ rewards ($* gamma maxqs donesf))
                                        :for qsa = ($gather ($execute model states) 1 actions)
                                        :do (let* ((loss (loss qsa tqs))
                                                   (lv (format nil "~,4F" ($data loss)))
                                                   (lbl (format nil "[~4D/~2D]" episode k)))
                                              (when (zerop (rem episode 100))
                                                (when (eq k 1) (prn lbl lv))
                                                (when (eq k ntrain) (prn lbl lv)))
                                              ($rmgd! model lr)))))))
                          (setf state next-state
                                done terminalp)))
              (when (zerop (rem episode 100))
                (let ((done nil)
                      (state (env/reset! env))
                      (r 0D0))
                  (loop :while (not done)
                        :for i :from 0 :below 100
                        :for action = (-> (epsilon-greedy model ($reshape state 1 4))
                                          ($ 0 0))
                        :do (let* ((tx (env/step! env action))
                                   (next-state (transition/next-state tx))
                                   (reward (transition/reward tx))
                                   (terminalp (transition/terminalp tx)))
                              (incf r reward)
                              (setf state next-state
                                    done terminalp)))
                  (prn "EVAL:" r done))))))
