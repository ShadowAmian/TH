(defpackage :gdrl-ch07
  (:use #:common-lisp
        #:mu
        #:th
        #:th.env)
  (:import-from #:th.env.examples))

(in-package :gdrl-ch07)

(defun decay-schedule (v0 minv decay-ratio max-steps &key (log-start -2) (log-base 10))
  (let* ((decay-steps (round (* max-steps decay-ratio)))
         (rem-steps (- max-steps decay-steps))
         (vs (-> ($/ (logspace log-start 0 decay-steps) (log log-base 10))
                 ($list)
                 (reverse)
                 (tensor)))
         (minvs ($min vs))
         (maxvs ($max vs))
         (rngv (- maxvs minvs))
         (vs ($/ ($- vs minvs) rngv))
         (vs ($+ minv ($* vs (- v0 minv)))))
    ($cat vs ($fill! (tensor rem-steps) ($last vs)))))

(let* ((env (th.env.examples:slippery-walk-seven-env))
       (optres (env/value-iteration env :gamma 0.99D0))
       (opt-v (value-iteration/optimal-value-function optres))
       (opt-p (value-iteration/optimal-policy optres))
       (opt-q (value-iteration/optimal-action-value-function optres)))
  (env/print-state-value-function env opt-v :ncols 9)
  (env/print-policy env opt-p :action-symbols '("<" ">") :ncols 9)
  (prn opt-q))

(defun sarsa-lambda (env &key (gamma 1D0)
                           (alpha0 0.5) (min-alpha 0.01) (alpha-decay-ratio 0.5)
                           (epsilon0 1.0) (min-epsilon 0.1) (epsilon-decay-ratio 0.9)
                           (lm 0.5) (replacing-traces-p T)
                           (nepisodes 3000))
  (let* ((ns (env/state-count env))
         (na (env/action-count env))
         (pi-track '())
         (Q (zeros ns na))
         (Q-track (zeros nepisodes ns na))
         (E (zeros ns na))
         (alphas (decay-schedule alpha0 min-alpha alpha-decay-ratio nepisodes))
         (epsilons (decay-schedule epsilon0 min-epsilon epsilon-decay-ratio nepisodes))
         (select-action (lambda (Q state epsilon)
                          (if (> (random 1D0) epsilon)
                              ($argmax ($ Q state))
                              (random ($count ($ Q state)))))))
    (loop :for k :from 0 :below nepisodes
          :for state = (env/reset! env)
          :for eps = ($ epsilons k)
          :for alpha = ($ alphas k)
          :for action = (funcall select-action Q state eps)
          :do (let ((done nil))
                ($zero! E)
                (loop :while (not done)
                      :do (let* ((tx (env/step! env action))
                                 (next-state (transition/next-state tx))
                                 (reward (transition/reward tx))
                                 (terminalp (transition/terminalp tx))
                                 (next-action (funcall select-action Q next-state eps))
                                 (fac (if terminalp 0 1))
                                 (td-target (+ reward (* gamma ($ Q next-state next-action)
                                                         fac)))
                                 (td-error (- td-target ($ Q state action))))
                            (when replacing-traces-p ($zero! ($ E state)))
                            (incf ($ E state action))
                            (when replacing-traces-p ($clamp! E 0 1))
                            (setf Q ($+ Q ($* alpha td-error E)))
                            (setf E ($* gamma lm E))
                            (setf done terminalp
                                  state next-state
                                  action next-action)))
                (setf ($ Q-track k) Q)
                (push ($squeeze ($argmax Q 1)) pi-track)))
    (let ((v ($squeeze (car ($max Q 1))))
          (va ($squeeze ($argmax Q 1))))
      (list Q v (lambda (s) ($ va s)) Q-track (reverse pi-track)))))

(let* ((env (th.env.examples:slippery-walk-seven-env))
       (res (sarsa-lambda env :gamma 0.99D0 :nepisodes 3000))
       (Q ($ res 0))
       (v ($ res 1))
       (policy ($ res 2)))
  (env/print-state-value-function env v :ncols 9)
  (env/print-policy env policy :action-symbols '("<" ">") :ncols 9)
  (prn Q))

(let* ((env (th.env.examples:slippery-walk-seven-env))
       (res (sarsa-lambda env :gamma 0.99D0 :replacing-traces-p nil :nepisodes 3000))
       (Q ($ res 0))
       (v ($ res 1))
       (policy ($ res 2)))
  (env/print-state-value-function env v :ncols 9)
  (env/print-policy env policy :action-symbols '("<" ">") :ncols 9)
  (prn Q))

(defun q-lambda (env &key (gamma 1D0)
                       (alpha0 0.5) (min-alpha 0.01) (alpha-decay-ratio 0.5)
                       (epsilon0 1.0) (min-epsilon 0.1) (epsilon-decay-ratio 0.9)
                       (lm 0.5) (replacing-traces-p T)
                       (nepisodes 3000))
  (let* ((ns (env/state-count env))
         (na (env/action-count env))
         (pi-track '())
         (Q (zeros ns na))
         (Q-track (zeros nepisodes ns na))
         (E (zeros ns na))
         (alphas (decay-schedule alpha0 min-alpha alpha-decay-ratio nepisodes))
         (epsilons (decay-schedule epsilon0 min-epsilon epsilon-decay-ratio nepisodes))
         (select-action (lambda (Q state epsilon)
                          (if (> (random 1D0) epsilon)
                              ($argmax ($ Q state))
                              (random ($count ($ Q state)))))))
    (loop :for k :from 0 :below nepisodes
          :for state = (env/reset! env)
          :for eps = ($ epsilons k)
          :for alpha = ($ alphas k)
          :for action = (funcall select-action Q state eps)
          :do (let ((done nil))
                ($zero! E)
                (loop :while (not done)
                      :do (let* ((tx (env/step! env action))
                                 (next-state (transition/next-state tx))
                                 (reward (transition/reward tx))
                                 (terminalp (transition/terminalp tx))
                                 (next-action (funcall select-action Q next-state eps))
                                 (maxa ($max ($ Q next-state)))
                                 (greedy-p (eq ($ Q next-state next-action) maxa))
                                 (fac (if terminalp 0 1))
                                 (td-target (+ reward (* gamma maxa fac)))
                                 (td-error (- td-target ($ Q state action))))
                            (when replacing-traces-p ($zero! ($ E state)))
                            (incf ($ E state action))
                            (when replacing-traces-p ($clamp! E 0 1))
                            (setf Q ($+ Q ($* alpha td-error E)))
                            (if greedy-p
                                (setf E ($* gamma lm E))
                                ($zero! E))
                            (setf done terminalp
                                  state next-state
                                  action next-action)))
                (setf ($ Q-track k) Q)
                (push ($squeeze ($argmax Q 1)) pi-track)))
    (let ((v ($squeeze (car ($max Q 1))))
          (va ($squeeze ($argmax Q 1))))
      (list Q v (lambda (s) ($ va s)) Q-track (reverse pi-track)))))

(let* ((env (th.env.examples:slippery-walk-seven-env))
       (res (q-lambda env :gamma 0.99D0 :nepisodes 3000))
       (Q ($ res 0))
       (v ($ res 1))
       (policy ($ res 2)))
  (env/print-state-value-function env v :ncols 9)
  (env/print-policy env policy :action-symbols '("<" ">") :ncols 9)
  (prn Q))

(let* ((env (th.env.examples:slippery-walk-seven-env))
       (res (q-lambda env :gamma 0.99D0 :replacing-traces-p nil :nepisodes 3000))
       (Q ($ res 0))
       (v ($ res 1))
       (policy ($ res 2)))
  (env/print-state-value-function env v :ncols 9)
  (env/print-policy env policy :action-symbols '("<" ">") :ncols 9)
  (prn Q))

(defun dyna-Q (env &key (gamma 1D0)
                     (alpha0 0.5) (min-alpha 0.01) (alpha-decay-ratio 0.5)
                     (epsilon0 1.0) (min-epsilon 0.1) (epsilon-decay-ratio 0.9)
                     (nplanning 3)
                     (nepisodes 3000))
  (let* ((ns (env/state-count env))
         (na (env/action-count env))
         (pi-track '())
         (T-track '())
         (R-track '())
         (planning-track '())
         (Q (zeros ns na))
         (T-count (tensor.int (zeros ns na ns)))
         (R-model (zeros ns na ns))
         (Q-track (zeros nepisodes ns na))
         (alphas (decay-schedule alpha0 min-alpha alpha-decay-ratio nepisodes))
         (epsilons (decay-schedule epsilon0 min-epsilon epsilon-decay-ratio nepisodes))
         (select-action (lambda (Q state epsilon)
                          (if (> (random 1D0) epsilon)
                              ($argmax ($ Q state))
                              (random ($count ($ Q state))))))
         (states (loop :for s :from 0 :below ns :collect s)))
    (loop :for k :from 0 :below nepisodes
          :for state = (env/reset! env)
          :for eps = ($ epsilons k)
          :for alpha = ($ alphas k)
          :do (let ((done nil))
                (loop :while (not done)
                      :do (let* ((action (funcall select-action Q state eps))
                                 (tx (env/step! env action))
                                 (next-state (transition/next-state tx))
                                 (reward (transition/reward tx))
                                 (terminalp (transition/terminalp tx))
                                 (R-diff (- reward ($ R-model state action next-state)))
                                 (fac (if terminalp 0 1))
                                 (maxa ($max ($ Q next-state)))
                                 (td-target (+ reward (* gamma maxa fac)))
                                 (td-error (- td-target ($ Q state action))))
                            (incf ($ T-count state action next-state))
                            (incf ($ R-model state action next-state)
                                  (/ R-diff ($ T-count state action next-state)))
                            (incf ($ Q state action) (* alpha td-error))
                            (loop :repeat nplanning
                                  :while (not (zerop ($sum Q)))
                                  :do (let* ((scnts ($gt ($squeeze ($sum ($sum T-count 2) 1)) 0))
                                             (acnts ($gt ($squeeze ($sum ($ T-count state) 1)) 0))
                                             (visited-states ($squeeze ($nonzero scnts)))
                                             (actions-taken ($squeeze ($nonzero acnts)))
                                             (state ($ visited-states
                                                      (random ($count visited-states))))
                                             (action ($ actions-taken
                                                       (random ($count actions-taken))))
                                             (next-state (let* ((tc ($ T-count state action))
                                                                (ss ($sum tc)))
                                                           (if (zerop ss)
                                                               (random ns)
                                                               ($choice states
                                                                        ($/ (tensor.float tc)
                                                                            ss)))))
                                             (reward ($ R-model state action next-state)))
                                        (push (list state action reward next-state)
                                              planning-track)
                                        (setf td-target (+ reward
                                                           (* gamma ($max ($ Q next-state)))))
                                        (setf td-error (- td-target ($ Q state action)))
                                        (incf ($ Q state action) (* alpha td-error))))
                            (setf done terminalp
                                  state next-state)))
                (push ($clone T-count) T-track)
                (push ($clone R-model) R-track)
                (setf ($ Q-track k) Q)
                (push ($squeeze ($argmax Q 1)) pi-track)))
    (let ((v ($squeeze (car ($max Q 1))))
          (va ($squeeze ($argmax Q 1))))
      (list Q v (lambda (s) ($ va s)) Q-track (reverse pi-track) (reverse T-track)
            (reverse R-track) (reverse planning-track)))))

(let* ((env (th.env.examples:slippery-walk-seven-env))
       (res (dyna-Q env :gamma 0.99D0 :nepisodes 3000))
       (Q ($ res 0))
       (v ($ res 1))
       (policy ($ res 2)))
  (env/print-state-value-function env v :ncols 9)
  (env/print-policy env policy :action-symbols '("<" ">") :ncols 9)
  (prn Q))
